0
00:00:00,500 --> 00:00:04,165
PROFESSOR IRIZARRY: We&#39;re ready to perform machine learning in R. So what

1
00:00:04,165 --> 00:00:07,790
we&#39;re going to try to do is we&#39;re going to try to predict tissues

2
00:00:07,790 --> 00:00:09,950
based on gene expression data.

3
00:00:09,950 --> 00:00:11,900
And as we&#39;ll see later, we&#39;re going to make

4
00:00:11,900 --> 00:00:14,200
it a little bit challenging because it turns out

5
00:00:14,200 --> 00:00:16,379
that that&#39;s a very easy thing to do.

6
00:00:16,379 --> 00:00:19,580
Tissues have very different gene expression profiles.

7
00:00:19,580 --> 00:00:24,210
So we&#39;ll try to do it with fewer dimensions than all the genes.

8
00:00:24,210 --> 00:00:27,592
Now, before we start, I want to show you the outcome

9
00:00:27,592 --> 00:00:28,800
that we&#39;re trying to predict.

10
00:00:28,800 --> 00:00:31,790
It&#39;s tissue.

11
00:00:31,790 --> 00:00:34,980
So we can see that there&#39;s one tissue placenta that only has six.

12
00:00:34,980 --> 00:00:38,380
So I&#39;m actually going to take that one out.

13
00:00:38,380 --> 00:00:42,420
And that way it gives us a little bit more balance.

14
00:00:42,420 --> 00:00:45,440
And it&#39;s better for illustration purposes.

15
00:00:45,440 --> 00:00:47,650
But there are ways to implement machine learning

16
00:00:47,650 --> 00:00:49,750
algorithms with cases like this.

17
00:00:49,750 --> 00:00:58,670
So let&#39;s take out the columns with that tissue that&#39;s your placenta.

18
00:00:58,670 --> 00:01:02,010
And then we will define y, our outcome of interest,

19
00:01:02,010 --> 00:01:05,300
and x, our matrix of predictors.

20
00:01:05,300 --> 00:01:08,450
So y is tissue categories.

21
00:01:08,450 --> 00:01:11,460
And x is the transpose of e.

22
00:01:11,460 --> 00:01:14,730
It&#39;s the transpose because each column now is a predictor.

23
00:01:14,730 --> 00:01:17,470
That&#39;s how these machine learning algorithms work.

24
00:01:17,470 --> 00:01:22,710
So we can see the dimension of x is the number of outcomes-- 183--

25
00:01:22,710 --> 00:01:24,700
that&#39;s the length of y.

26
00:01:24,700 --> 00:01:29,190
And it has 22,215 predictors.

27
00:01:29,190 --> 00:01:34,410
Now, the first thing we think about when we&#39;re

28
00:01:34,410 --> 00:01:38,380
getting ready to do machine learning is cross validation.

29
00:01:38,380 --> 00:01:42,550
So it&#39;s very important that we cross validate our results that we

30
00:01:42,550 --> 00:01:45,270
train on one set and test on another.

31
00:01:45,270 --> 00:01:49,330
So the first thing I&#39;m going to do is create a set of indices

32
00:01:49,330 --> 00:01:53,860
that facilitates cross validation.

33
00:01:53,860 --> 00:01:57,070
And we&#39;re going to use this nice, little function in the caret package

34
00:01:57,070 --> 00:01:58,590
to create folds.

35
00:01:58,590 --> 00:02:01,540
It automatically does this for us.

36
00:02:01,540 --> 00:02:07,390
So it takes our outcome y, and we&#39;re going to create five folds.

37
00:02:07,390 --> 00:02:10,919
So what it does is it splits the data into five groups.

38
00:02:10,919 --> 00:02:13,730
And now what we&#39;re going to do, which are called the folds.

39
00:02:13,730 --> 00:02:18,230
And what we&#39;re going to do when we come to the prediction part

40
00:02:18,230 --> 00:02:21,840
is we&#39;re going to leave out one fold, use the other four to train,

41
00:02:21,840 --> 00:02:23,920
and then test on that one we left out.

42
00:02:23,920 --> 00:02:26,130
That&#39;s the idea behind cross validation.

43
00:02:26,130 --> 00:02:28,100
And then we do that for each fold.

44
00:02:28,100 --> 00:02:32,260
We leave out the first, the second, the third, the fourth, and the fifth.

45
00:02:32,260 --> 00:02:35,110
Let&#39;s show you what the outcomes are in these five folds.

46
00:02:35,110 --> 00:02:36,230
So have five folds.

47
00:02:36,230 --> 00:02:40,710
And in each one, we have a series of tissues.

48
00:02:40,710 --> 00:02:44,267
These are the tissues in the five subsets.

49
00:02:44,267 --> 00:02:46,350
The number of tissues in each of the five subsets.

50
00:02:46,350 --> 00:02:48,560
You can see why we left placenta out.

51
00:02:48,560 --> 00:02:50,670
A lot of these would have zero placentas.

52
00:02:50,670 --> 00:02:55,510
And it would complicate the problem slightly.

53
00:02:55,510 --> 00:02:59,900
Now, as I said earlier, we&#39;re going to make this problem more challenging

54
00:02:59,900 --> 00:03:01,840
by looking at only two dimensions.

55
00:03:01,840 --> 00:03:04,650
We&#39;re going to take the first two dimensions that

56
00:03:04,650 --> 00:03:08,510
is given to us by cmdscale.

57
00:03:08,510 --> 00:03:14,240
So we&#39;re going to make a smaller set of predictors, basically, two.

58
00:03:14,240 --> 00:03:18,670
And we&#39;re going to try to predict tissues based on these two predictors.

59
00:03:18,670 --> 00:03:25,030
So I&#39;m going to load the rafalib package to make some nice looking pictures.

60
00:03:25,030 --> 00:03:30,940
And you can see what these two dimensions are.

61
00:03:30,940 --> 00:03:32,420
And there they are.

62
00:03:32,420 --> 00:03:35,220
And we have the different colors or the different tissues.

63
00:03:35,220 --> 00:03:39,090
So we&#39;re going to try to predict those colors based on these two dimensions.

64
00:03:39,090 --> 00:03:42,240
You can already see that it&#39;s not going to be easy, because they

65
00:03:42,240 --> 00:03:44,730
don&#39;t separate out perfect.

66
00:03:44,730 --> 00:03:46,730
All right, so let&#39;s get started.

67
00:03:46,730 --> 00:03:53,090
So I&#39;m going to use knn to do this, which is in the library class.

68
00:03:53,090 --> 00:03:58,230
And I&#39;m going to apply knn in the following way.

69
00:03:58,230 --> 00:04:07,367
What I do is I-- I&#39;m going to train on the rows-- on all the rows except the ith

70
00:04:07,367 --> 00:04:07,900
fold.

71
00:04:07,900 --> 00:04:09,880
So here I&#39;m doing the first one.

72
00:04:09,880 --> 00:04:13,570
So I&#39;m going to take out the rows that are

73
00:04:13,570 --> 00:04:17,870
indexed in that first fold-- take them out-- take them out also

74
00:04:17,870 --> 00:04:18,830
for the outcome y.

75
00:04:18,829 --> 00:04:21,970
You can see here-- cl gives us the outcome.

76
00:04:21,970 --> 00:04:27,250
And then we&#39;re going to test on the remaining dataset.

77
00:04:27,250 --> 00:04:31,730
So that gives us back-- we&#39;re going to get back results for this test data

78
00:04:31,730 --> 00:04:32,230
as well.

79
00:04:32,230 --> 00:04:41,580
So we don&#39;t use the indices in this fold to train the model.

80
00:04:41,580 --> 00:04:42,960
So that&#39;s our test set.

81
00:04:42,960 --> 00:04:45,610
So we can run this quick.

82
00:04:45,610 --> 00:04:47,040
It&#39;s very fast.

83
00:04:47,040 --> 00:04:48,690
And now we can see how we did.

84
00:04:48,690 --> 00:04:54,962
So we can look at the tissues and the actual tissues and the calls.

85
00:04:54,962 --> 00:04:57,920
And we can see, for example, that we made some mistakes for cerebellum.

86
00:04:57,920 --> 00:05:00,300
There&#39;s two mistakes here.

87
00:05:00,300 --> 00:05:02,420
I think we got all our colons right.

88
00:05:02,420 --> 00:05:05,830
But we got some errors in the hippocampus.

89
00:05:05,830 --> 00:05:10,520
We can just tally up all the mistakes we made here.

90
00:05:10,520 --> 00:05:12,065
We count how many mistakes we made.

91
00:05:12,065 --> 00:05:12,940
And we take the mean.

92
00:05:12,940 --> 00:05:19,250
And we see that 17%-- we made mistakes in 17% of our predictions.

93
00:05:19,250 --> 00:05:21,630
Now that was only one fold.

94
00:05:21,630 --> 00:05:25,700
So now the next step is to run through this,

95
00:05:25,700 --> 00:05:30,900
do the same at the same thing we just did-- do it now for every fold.

96
00:05:30,900 --> 00:05:32,730
So that&#39;s what I&#39;m going to do now.

97
00:05:32,730 --> 00:05:36,810
I&#39;m going to go along each of the five folds.

98
00:05:36,810 --> 00:05:38,940
This is just one through five.

99
00:05:38,940 --> 00:05:40,100
See that there.

100
00:05:40,100 --> 00:05:42,830
And for each one, I&#39;m going to run again--

101
00:05:42,830 --> 00:05:45,950
what we just did-- I&#39;m going to run it again leaving out now

102
00:05:45,950 --> 00:05:48,530
the first, second, third, fourth and fifth fold

103
00:05:48,530 --> 00:05:51,400
and then counting the number of mistakes we made.

104
00:05:51,400 --> 00:05:52,240
So we just run that.

105
00:05:52,240 --> 00:05:56,070
I&#39;m going to do that for the first-- so I don&#39;t this i anymore.

106
00:05:58,870 --> 00:05:59,700
Where did I go?

107
00:05:59,700 --> 00:06:02,300
I&#39;m sorry-- right down here.

108
00:06:02,300 --> 00:06:06,110
So I don&#39;t need to-- the i is set in sapply.

109
00:06:06,110 --> 00:06:09,780
And I do need to set k because I am naming in general.

110
00:06:09,780 --> 00:06:13,160
You&#39;ll see why we&#39;re doing that in a second to let&#39;s say five.

111
00:06:13,160 --> 00:06:14,160
Now we run it.

112
00:06:14,160 --> 00:06:17,490
Now we are doing, again, we&#39;re doing the cross validation.

113
00:06:17,490 --> 00:06:20,570
Basically, we&#39;re testing and training, leaving out folds,

114
00:06:20,570 --> 00:06:23,190
and we&#39;re doing it for all five.

115
00:06:23,190 --> 00:06:25,640
We run this-- should be pretty quick.

116
00:06:25,640 --> 00:06:29,800
And now we have in res.k-- the results.k--

117
00:06:29,800 --> 00:06:33,280
we can see we made six mistakes in each one of them.

118
00:06:36,070 --> 00:06:44,540
Now the next step is to compare the different k.

119
00:06:44,540 --> 00:06:50,240
So we want to choose the number of nearest neighbors that we include.

120
00:06:50,240 --> 00:06:51,557
Is 5 a good choice?

121
00:06:51,557 --> 00:06:52,640
Or should we have done 10?

122
00:06:52,640 --> 00:06:54,240
Shoud we have done 1?

123
00:06:54,240 --> 00:06:57,610
So before we continue with that, let&#39;s write down

124
00:06:57,610 --> 00:07:00,130
here what the final outcome is.

125
00:07:00,130 --> 00:07:04,530
We sum up res.k and divide by the length of y.

126
00:07:04,530 --> 00:07:10,990
And that gives us the error rate-- 16% now.

127
00:07:10,990 --> 00:07:15,870
I&#39;m going to do this now-- not for every single k, but several k.

128
00:07:15,870 --> 00:07:18,040
So let&#39;s do 1 through 12.

129
00:07:18,040 --> 00:07:24,810
And now what I do is I add another level of another loop.

130
00:07:24,810 --> 00:07:28,960
I go through the k&#39;s-- function(k).

131
00:07:28,960 --> 00:07:37,020
And I&#39;m ready to apply what we just did for these 12 values of k.

132
00:07:37,020 --> 00:07:41,620
And then we can plot them and see which one works best.

133
00:07:41,620 --> 00:07:42,600
Then we close it.

134
00:07:42,600 --> 00:07:44,590
All right, so there it&#39;s pretty fast.

135
00:07:44,590 --> 00:07:52,820
And now we can just plot k&#39;s against this res and see how they compare.

136
00:07:52,820 --> 00:07:56,310
And we can see that the best choice actually was five.

137
00:07:56,310 --> 00:07:59,360
And we can see that 0 makes more mistakes.

138
00:07:59,360 --> 00:08:00,380
And then it goes up.

139
00:08:00,380 --> 00:08:07,100
So that&#39;s how we use cross validation to do the prediction.

140
00:08:07,100 --> 00:08:10,870
And with just two dimensions, we did OK.

141
00:08:10,870 --> 00:08:14,220
16% mistakes is not terrible.

142
00:08:14,220 --> 00:08:16,410
Now the last thing I want to show you here

143
00:08:16,410 --> 00:08:25,530
is that we use more-- if we use more samples-- sorry, if we

144
00:08:25,530 --> 00:08:32,539
use more than two dimensions, then it becomes pretty easy to do this.

145
00:08:32,539 --> 00:08:33,549
So we can see that.

146
00:08:33,549 --> 00:08:38,919
If I now redo it with five dimensions instead of two,

147
00:08:38,919 --> 00:08:48,390
we can see that-- let&#39;s wait for it to finish and then run it.

148
00:08:48,390 --> 00:08:50,210
So we use five dimensions.

149
00:08:50,210 --> 00:08:54,460
Before we used two in the k nearest neighbors approach.

150
00:08:54,460 --> 00:08:56,720
Then we do much, much better.

151
00:08:56,720 --> 00:09:00,950
We basically make no mistakes when we have more dimensions.

152
00:09:00,950 --> 00:09:05,110
So just with five dimensions, we&#39;re already doing a very good job.

153
00:09:05,110 --> 00:09:12,480
If we make it 100 or 1,000 dimensions, then we&#39;ll probably do perfect.

154
00:09:12,480 --> 00:09:17,370
So that&#39;s how you use cross validation to test out machine learning procedures

155
00:09:17,370 --> 00:09:19,937
and also to pick the parameters.

