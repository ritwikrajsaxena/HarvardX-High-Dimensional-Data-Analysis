0
00:00:00,000 --> 00:00:01,240


1
00:00:01,240 --> 00:00:06,580
RAFAEL IRIZARRY: In this module we're going to describe another clustering algorithm called

2
00:00:06,580 --> 00:00:08,920
k-means.

3
00:00:08,920 --> 00:00:13,390
K-means, just like the other algorithms, starts

4
00:00:13,390 --> 00:00:19,520
by computing a distance between each unit in our data set.

5
00:00:19,520 --> 00:00:27,125
So in this plot, we can see that we have about 150 different samples.

6
00:00:27,125 --> 00:00:31,690
And we are working in two dimensions, dimension 1 on the x-axis,

7
00:00:31,690 --> 00:00:34,830
dimension 2 on the y-axis.

8
00:00:34,830 --> 00:00:40,330
And we want the algorithm to tell us which samples are in which clusters.

9
00:00:40,330 --> 00:00:42,200
Another thing we have to do with k-means is

10
00:00:42,200 --> 00:00:44,470
decide on the number of clusters we want at the end.

11
00:00:44,470 --> 00:00:46,360
That's what the k stands for.

12
00:00:46,360 --> 00:00:48,650
K is the number of clusters.

13
00:00:48,650 --> 00:00:53,090
So that is predetermined by the user.

14
00:00:53,090 --> 00:00:58,940
There are techniques to pick among different results from different Ks.

15
00:00:58,940 --> 00:01:02,770
But you still have to start each one by defining k.

16
00:01:02,770 --> 00:01:07,010
So for this example, we're going to use three, for this illustration.

17
00:01:07,010 --> 00:01:10,710
So we're going to tell k-means there are three clusters.

18
00:01:10,710 --> 00:01:15,550
So now the algorithm automatically has to put each sample

19
00:01:15,550 --> 00:01:17,200
into one of three clusters.

20
00:01:17,200 --> 00:01:20,920
Notice that for us, just by looking at this,

21
00:01:20,920 --> 00:01:23,780
we can see what the answer should be.

22
00:01:23,780 --> 00:01:26,220
You can see there are three clusters very clear.

23
00:01:26,220 --> 00:01:27,910
So you can say, here, here, and here.

24
00:01:27,910 --> 00:01:32,060
However, we want the computer to do this.

25
00:01:32,060 --> 00:01:35,650
In practice we might have to do thousands of these.

26
00:01:35,650 --> 00:01:37,664
And we can't look at each one separately.

27
00:01:37,664 --> 00:01:39,580
So that's how methods like this can be useful,

28
00:01:39,580 --> 00:01:41,560
even for simple data sets like this.

29
00:01:41,560 --> 00:01:46,320
So the first step is to pick three centroids, three centers for the three

30
00:01:46,320 --> 00:01:47,540
clusters.

31
00:01:47,540 --> 00:01:48,690
How do we do that?

32
00:01:48,690 --> 00:01:52,680
The standard approach in k-means is to pick three samples at random.

33
00:01:52,680 --> 00:01:56,110
So we do that and we see the Xs.

34
00:01:56,110 --> 00:02:00,930
The Xs are the three points that were selected at random.

35
00:02:00,930 --> 00:02:05,530
Now for each point, for each point in the space,

36
00:02:05,530 --> 00:02:08,990
we figure out which of those 3 Xs is closest.

37
00:02:08,990 --> 00:02:12,550
If the closest X, if the closest center is orange,

38
00:02:12,550 --> 00:02:15,100
then we make the sample orange.

39
00:02:15,100 --> 00:02:19,970
If the closest X is green, then we make the sample green.

40
00:02:19,970 --> 00:02:25,350
So now we have a cluster assignment for each point.

41
00:02:25,350 --> 00:02:27,550
Then on the next step we do it again.

42
00:02:27,550 --> 00:02:32,780
But before finding new clusters, we're going to redefine the centroids.

43
00:02:32,780 --> 00:02:33,690
How?

44
00:02:33,690 --> 00:02:38,060
We're going to use the current approximation of the clusters

45
00:02:38,060 --> 00:02:39,800
to define a new center.

46
00:02:39,800 --> 00:02:44,960
So we take all the green points and we compute the dimension 1 mean,

47
00:02:44,960 --> 00:02:47,700
dimension 2 mean, and that is the new centroid.

48
00:02:47,700 --> 00:02:48,660
So now here it is.

49
00:02:48,660 --> 00:02:50,480
Here's the new X for the greens.

50
00:02:50,480 --> 00:02:51,720
It's over here.

51
00:02:51,720 --> 00:02:53,660
The new X for the purples is over here.

52
00:02:53,660 --> 00:02:56,820
And the new X for the oranges are over here.

53
00:02:56,820 --> 00:02:59,840
And now we redefine clusters again.

54
00:02:59,840 --> 00:03:03,610
Just find the X that's closest and use that color.

55
00:03:03,610 --> 00:03:04,870
So here's the next step.

56
00:03:04,870 --> 00:03:06,270
That didn't move much.

57
00:03:06,270 --> 00:03:07,700
We keep doing it.

58
00:03:07,700 --> 00:03:10,450
We keep doing it and doing it.

59
00:03:10,450 --> 00:03:12,980
And we see that the Xs are slowly moving.

60
00:03:12,980 --> 00:03:16,500
And now they're getting closer to the center where we think they should be.

61
00:03:16,500 --> 00:03:20,150
And by the end, we have a pretty good result.

62
00:03:20,150 --> 00:03:22,870
So here it is again in a movie.

63
00:03:22,870 --> 00:03:27,030
You can see how in each step we get closer and closer to the center

64
00:03:27,030 --> 00:03:30,005
that we expect to be the correct answer.

65
00:03:30,005 --> 00:03:33,300
It's important to keep in mind that k-means

66
00:03:33,300 --> 00:03:36,540
selects its starting values at random.

67
00:03:36,540 --> 00:03:40,540
Therefore, every time you run it, you might get a different answer.

68
00:03:40,540 --> 00:03:44,430
It's also important to keep in mind that because you

69
00:03:44,430 --> 00:03:47,080
can select bad starting values.

70
00:03:47,080 --> 00:03:51,869
You can sometimes end up with a pretty bad final result.

71
00:03:51,869 --> 00:03:54,160
So these are things that are important to keep in mind.

72
00:03:54,160 --> 00:03:57,680
This is a powerful algorithm that works very well in general.

73
00:03:57,680 --> 00:04:00,510
But it does have its deficiencies.

74
00:04:00,510 --> 00:04:03,940
And it's sometimes useful to explore the data,

75
00:04:03,940 --> 00:04:08,717
explore the final answer to make sure that the algorithm worked well.

76
00:04:08,717 --> 00:04:09,216


